{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Multi-class compound structural classification using sklearn</center></h1>\n",
    "\n",
    "## About our dataset for today\n",
    "\n",
    "The objective of this exercise is to predict the structural category of a compound using some properties derived from the monoisotopic mass and the molecular formula of the compound. ~40,000 plant lipids were classified in the following eight categories, out of which ~8000 lipids have been randomly chosen for this exercise.\n",
    "\n",
    "<img src=\"Lipids.jpg\" height=\"700\" width=\"700\">\n",
    "\n",
    "We first used Weka for classification. Although Weka also has a command line interface and is quite easy to use, its functionality, especially for multiclass classification, is limited. SkLearn has a lot more functionality than Weka, is much more powerful in its libraries, options and execution, and is more actively developed.\n",
    "\n",
    "The overall pipeline for analysis of the lipid dataset is as follows:\n",
    "\n",
    "* We will first perform some exploratory analysis of the data to understand the distribution of the various features. This dataset has been filtered properly already, so there's not much to edit post exploratory analysis.\n",
    "\n",
    "* We will divide the dataset into training and test data, and set up machine learning runs with 10X cross-validation. We will build a model with a Random Forest classifier using training data and use it to predict the class of the test dataset examples\n",
    "\n",
    "* Then, we will estimate the accuracy of the classifier using various metrics.\n",
    "\n",
    "* Finally, we will determine which features were important for classification purposes\n",
    "\n",
    "If at any point you wish to determine the effect of changing some parameters on the output, or want to print some internal variables, feel free to edit the code cell and re-run the cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory analysis of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib notebook\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "#Read in the file\n",
    "s = pd.read_csv('small_input_file.tab',sep=\"\\t\")\n",
    "s.head()\n",
    "\n",
    "#How many columns does it have? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get some basic numerical descriptors of the various columns\n",
    "#Uncomment each of these lines one at a time\n",
    "print (s.info())\n",
    "print (s.describe())\n",
    "\n",
    "#REMEMBER: Categorical features CANNOT be used for machine learning unless you transform them somehow \n",
    "# into numerical features. \n",
    "\n",
    "#Questions:\n",
    "#1) What are categorical features?\n",
    "#2) Which features in this dataset can be used for machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the distributions of the various features\n",
    "s.hist(bins=20,figsize=(15,15), color='#86bf91');\n",
    "\n",
    "#What is matplotlib plotting here? How does it know what values to make histograms from?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 1:__ \n",
    "\n",
    "Notice the ; at the end of the above command. Remove it and rerun code. What happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next we look at correlations between the different variables. \n",
    "allcorrel=s.corr()\n",
    "allcorrel.columns\n",
    "#len(allcorrel.columns)\n",
    "allcorrel\n",
    "\n",
    "#Why do you need to do this analysis? What will happen if correlated variables are included for training \n",
    "#the machine learning models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us first plot a basic heatmap\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(allcorrel,cmap='plasma',origin='lower')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this does not make good X-axis and Y-axis labels. We need to provide them separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the next command, it is important to first understand what allcorrel.columns is\n",
    "print (\">\", allcorrel.columns)\n",
    "print (\">>>\", len(allcorrel.columns))\n",
    "print (\">>>>>\", np.arange(len(allcorrel.columns)))\n",
    "\n",
    "#We will use these values for plotting a better heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create empty plot variables\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "#Lets first set the ax\n",
    "ax.imshow(allcorrel,cmap='plasma',origin='lower') #other color options: viridis, inferno, hot, magma, cividis etc\n",
    "#https://matplotlib.org/tutorials/colors/colormaps.html\n",
    "\n",
    "ax.set_xticks(np.arange(len(allcorrel.columns))) #This is the >>>>> above\n",
    "ax.set_yticks(np.arange(len(allcorrel.columns))) #This is the >>>>> above\n",
    "ax.set_xticklabels(allcorrel.columns) #X-axis labels > above\n",
    "ax.set_yticklabels(allcorrel.columns) #Y-axis labels > above\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\",size=12)\n",
    "plt.setp(ax.get_yticklabels(), rotation=0, ha=\"right\", rotation_mode=\"anchor\",size=12)\n",
    "ax.set_title(\"Pairwise Pearsons Correlation Coefficients between features\")\n",
    "\n",
    "#Now set the fig\n",
    "fig.set_size_inches(8,8)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seaborn, which is a wrapper built around matplotlib libraries, solves this problem of code complexity by making the whole block above basically a 4-line code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seaborn code\n",
    "plt.figure(figsize=(12,12))\n",
    "ax = sns.heatmap(allcorrel, cmap=\"plasma\")\n",
    "plt.xticks(rotation=45)\n",
    "ax.invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__POP QUIZ:__ What does the above figure show?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up and executing the machine learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required modules\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#Example Documentation: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, define which columns are labels and which are features\n",
    "labels=s['Category']\n",
    "feats=s[s.columns[5:]] #Are these the correct columns? Go back to the table where you defined s and check.\n",
    "annot=s[s.columns[0:5]]\n",
    "\n",
    "#Additionally, uncomment each line below to print only the top 5 rows of each variable to make sure correct columns \n",
    "#were indeed pulled out\n",
    "print (feats.head())\n",
    "print (labels.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variation 1: Splitting data using train-test-split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split your data into training and test sets - here we are specifying the size of the test set as 0.2 (20%) of the main dataset, which has 8040 entries. We have to specify training and test labels as well as features. Check if the split is really 80:20.\n",
    "\n",
    "Also note that we are not doing any Kfold cross-validation. If the dataset is sufficiently large, you can train the model on the training and test it on a sufficiently large test set. This is not advisable, but is the simplest form of training that can be done. No cross-validation required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feats, test_feats, train_labels, test_labels  = train_test_split (feats, labels, test_size=0.2)\n",
    "print (test_labels.shape)\n",
    "print (train_labels.shape)\n",
    "print (test_feats.shape)\n",
    "print (train_feats.shape)\n",
    "\n",
    "#Does this look ok?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets make a function here to train our machine learning model\n",
    "#We are running a multi-class classifier here, instead of a binary classifier (yes, no)\n",
    "def random_forest_classifier(features, target):\n",
    "    \"\"\"\n",
    "    To train the random forest classifier with features and target data\n",
    "    :param features: features\n",
    "    :param target: labels\n",
    "    :return: trained random forest classifier\n",
    "    \"\"\"\n",
    "    clf = RandomForestClassifier(n_estimators=100)\n",
    "    #estimators stands for the number of decision trees you want to implement\n",
    "    #Documentation: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "    clf.fit(features, target)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running the randomForestClassifier\n",
    "trained_model = random_forest_classifier(train_feats, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's see if the trained model predicts the test features correctly. \n",
    "pred_labels=trained_model.predict(test_feats)\n",
    "pred_probs=trained_model.predict_proba(test_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Uncomment each line below to understand what's going on\n",
    "\n",
    "trained_model.__dict__ #This just shows you the model, and in this case we are only using default values, so its showing up as ().\n",
    "#pred_labels\n",
    "#pred_probs #Probabilities of each test instance belonging to each class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variation 2: Using KFold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(5, True, 1) #An empty splitter object for 5-fold cross validation\n",
    "\n",
    "#Setup the basic model\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "#Train the model using cross-validation\n",
    "scores = cross_val_score(rf, train_feats, train_labels, scoring='accuracy', cv=kfold, n_jobs=-1, verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variation 3: Using grid search across all RandomForest parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "#Set up the parameters for grid search\n",
    "grid_param = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "#Create an empty model\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "kfold = KFold(5, True, 1)\n",
    "\n",
    "#Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=grid_param, scoring='accuracy', cv=kfold, n_jobs=-1)\n",
    "\n",
    "#Fit the model to the data\n",
    "trained_model=grid_search.fit(train_feats, train_labels)\n",
    "\n",
    "#See all properties of trained_model\n",
    "print (trained_model.__dict__)\n",
    "\n",
    "#Print best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print (\">>>>>\", best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK - now we are proceeding with the best model from grid search and seeing how it behaves with the test dataset. We will:\n",
    "* Make predictions on the test data\n",
    "* Output the predictions to a tab-delimited file\n",
    "* See if the file is correctly formatted\n",
    "* Fix any errors that occur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance on the test set\n",
    "pred_labels=trained_model.predict(test_feats)\n",
    "pred_probs=trained_model.predict_proba(test_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us output these predictions to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now check how many elements are contained within pred_labels. Is it a matrix of values?\n",
    "dfx = pd.concat([test_feats, pd.DataFrame(pred_labels, columns=[\"Predicted labels\"]), \n",
    "                 pd.DataFrame(pred_probs, columns=[\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\"])], axis=1)\n",
    "dfx.to_csv(path_or_buf=\"results.tab\",index=True,sep='\\t')\n",
    "\n",
    "#Check the output. Is it correct? You should scroll down to t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now check how many elements are contained within pred_labels. Is it a matrix of values?\n",
    "\n",
    "dfx1 = pd.concat([test_feats, pd.DataFrame(test_labels)], axis=1, sort=False)\n",
    "dfx = pd.concat([dfx1.reset_index(),\n",
    "                 pd.DataFrame(pred_labels, columns=[\"Predicted labels\"]), \n",
    "                 pd.DataFrame(pred_probs, columns=[\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\"])], axis=1)\n",
    "\n",
    "dfx.to_csv(path_or_buf=\"results2.tab\",index=True,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#why did we have to use such a complicated statement? Because there are no indexes available for the pred_labels/probs\n",
    "#to merge with the test_labels and test_feats datasets.\n",
    "\n",
    "#pred_probs\n",
    "#dfx1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx1['pred_labels']=pred_labels\n",
    "dfx1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with sklearn is it is not straightforward to export out the predictions of the model, since the indexes of the predictions (pred_labels, pred_probs) are all messed up. Hence, a recommended strategy is -- after we are satisfied with a model -- we run it against the whole dataset and identify the test examples separately. That is what we will do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the overall performance of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will determine:\n",
    "1. Accuracy of the model\n",
    "2. Area Under the Receiver Operating Characteristic\n",
    "3. Confusion matrix - how many instances are misclassified vs. correctly classified\n",
    "4. Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class_names=test_labels[unique_labels[test_labels, pred_labels]]\n",
    "class_names=list(set(train_labels))\n",
    "class_names\n",
    "\n",
    "#This is the complete, unique list of labels in the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "len(pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#Using this model, predict labels of test features and see how well it behaves\n",
    "for i in range(0, 20): #Change this number from 10 to something else (<20) and see what happens\n",
    "        print (\"Original label :: {} and Predicted label :: {}\".format(list(test_labels)[i], pred_labels[i]))\n",
    "print(\"Overall accuracy:\",accuracy_score(test_labels, pred_labels))\n",
    "\n",
    "#What do you think was printed here? \n",
    "#Uncheck the following command. Run the block again and check the output as well as the above command.\n",
    "#Can you tell now?\n",
    "\n",
    "print (\">>>\", len(list(test_labels)))\n",
    "\n",
    "#Confusion matrix\n",
    "class_names=list(set(train_labels))\n",
    "cm=confusion_matrix(test_labels, pred_labels, labels=class_names)\n",
    "print (cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalized confusion matrix\n",
    "#Here we are normalizing each of the value above by the total number of examples of each category in the test dataset\n",
    "cm=cm.astype('float')/cm.sum(axis=1)[:,np.newaxis]*100\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the confusion matrix\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm, interpolation='nearest', cmap=\"viridis\")\n",
    "ax.figure.colorbar(im, ax=ax)\n",
    "fig.set_size_inches(7,7)\n",
    "fig.tight_layout()\n",
    "\n",
    "# We want to show all ticks...\n",
    "ax.set(xticks=np.arange(cm.shape[1]),\n",
    "       yticks=np.arange(cm.shape[0]),\n",
    "       # ... and label them with the respective list entries\n",
    "       xticklabels=class_names, yticklabels=class_names,\n",
    "       title=\"Multi-class confusion matrix\",\n",
    "       ylabel='True label',\n",
    "       xlabel='Predicted label')\n",
    "#ax.tick_params(direction=2)\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1:\n",
    "\n",
    "* What does the plot show?\n",
    "* Which structural categories was the model able to classify properly, and where was it confused?\n",
    "* Why do you think the model was confused by those categories? What is it confused with? Why? Refer to the category definitions shown above to address these questions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Area Under the Receiver Operating Characteristic (AUROC) curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUROC curve is always in a binary context, while here we have run a multiclass classifier. \n",
    "Hence, it is important to format the AUROC calculations in a way that enables binary calculation/plotting. We will convert all labels into only two types -- Fatty Acyls and Not Fatty Acyls - and run the classifier again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the Area Under the Receiver Operating Characteristic (AUROC) curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "#Get the labels and features again, just so that we don't mess up the original variables\n",
    "labels2=s['Category']\n",
    "feats2=s[s.columns[5:]]\n",
    "train_feats2, test_feats2, train_labels2, test_labels2  = train_test_split (feats2, labels2, test_size=0.2)\n",
    "\n",
    "for name in class_names:\n",
    "    #print (name)\n",
    "    if name=='Fatty_Acyls':\n",
    "        pass\n",
    "    else:\n",
    "        train_labels2=pd.Series(train_labels2).str.replace(name,'Not_FA') #replace labels in training\n",
    "        test_labels2=pd.Series(test_labels2).str.replace(name,'Not_FA') #replace labels in test\n",
    "\n",
    "#Run Random Forest classifier on them and print the overall model score\n",
    "rf = RandomForestClassifier(n_estimators=100, oob_score=True) \n",
    "trained_model2 = rf.fit(train_feats2, train_labels2)\n",
    "model_scores2=rf.score(test_feats2, test_labels2)\n",
    "print (\"Overall model score: \", model_scores2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the Area Under the Receiver Operating Characteristic (AUC-ROC or AUROC)\n",
    "y_score=trained_model2.predict_proba(test_feats2)[:, 1]\n",
    "fpr, tpr, _ = roc_curve(test_labels2, y_score, pos_label='Fatty_Acyls')\n",
    "#Does the figure make sense? Can AUROC be lower than random expectation?\n",
    "\n",
    "#Change pos_label to Not_FA and see how the AUROC figure changes below.\n",
    "\n",
    "#This is a known issue with AUROC, occurring mostly in the context of multi-class classification. \n",
    "#Some wrapper packages get around this issue. Nonetheless, we can obtain correct AUROC plots by flipping the\n",
    "#class that is being plotted i.e. Not_FA instead of Fatty_Acyls\n",
    "\n",
    "\n",
    "roc_auc = auc(fpr, tpr)\n",
    "#print (fpr, tpr, roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(y_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the AUROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding which features were important for binary classification of Fatty Acyls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature importance is calculated as a specific metric, which is different for different algorithms. See a detailed review here: https://machinelearningmastery.com/calculate-feature-importance-with-python/. In our case, rf.feature_importance_ method by default uses the Gini impurity decrease method, which calculates the drop in \"impurity\" of the predicted labels, if individual features were removed from the nodes of the RF decision trees.\n",
    "\n",
    "There are some issues with this metric with certain types of datasets. Depending on your data structure, you may need to choose different feature importance measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feats2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature importance\n",
    "\n",
    "feature_importances = pd.DataFrame(rf.feature_importances_,\n",
    "                                   index = train_feats2.columns,\n",
    "                                    columns=['importance']).sort_values('importance', ascending=False)\n",
    "\n",
    "# Print the feature ranking\n",
    "print (feature_importances)\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "figcolumns=feature_importances.index.values\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(train_feats2.shape[1]), feature_importances.importance,\n",
    "       color=\"r\", align=\"center\")\n",
    "plt.ylabel(\"Feature importances\")\n",
    "plt.xticks(range(train_feats2.shape[1]), figcolumns, rotation='vertical')\n",
    "plt.xlim([-1, train_feats2.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Copy-paste the above plot into a Word File or a Powerpoint presentation__\n",
    "\n",
    "__Exercise 2__:\n",
    "\n",
    "* Do the feature importances make sense, based on what you know about Fatty Acyls?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing the predictions to output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the final step in our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats=s[s.columns[5:]]\n",
    "full_labels=trained_model.predict(feats)\n",
    "full_probs=trained_model.predict_proba(feats)\n",
    "feats['Predictions']=full_labels\n",
    "feats2=pd.concat([feats, annot, pd.DataFrame(pred_probs, columns=[\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\"])], axis=1)\n",
    "feats2.to_csv(path_or_buf=\"results3.tab\",index=True,sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HOMEWORK - 10 points\n",
    "\n",
    "Repeat the binary classification with Glycerophospholipids, and check which features are important. To clarify the question, you have already performed multi-class classification. You simply need to obtain AUROC and feature importances for Glycerophospholipids vs. Not_Glycerophospholipids. \n",
    "\n",
    "Make a single Jupyter Notebook with your name in the file name, and upload it on Canvas. Points will be given as follows:\n",
    "\n",
    "1. How easy is it to run the code? -- 2 points\n",
    "2. Is the code well-commented? -- 2 points\n",
    "3. Is the AUROC graph correct? -- 2 points\n",
    "4. Is the Feature Importance graph correct? -- 2 point\n",
    "5. Is there an interpretation provided for feature importance -- 2 points\n",
    "\n",
    "This exercise should take <15 minutes to complete and submit. Submit on Canvas by Wed morning 9am."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the coding required for Python SciKit for running ML is fairly complicated, compared to the graphical interface of Weka. However, SciKit is very powerful, and once you get past the learning curve, you will be able to quickly run different algorithms using the HPC environment. SciKit also allows encoding neural networks as well as multi-label classification (not multi-class) -- where individual instances can be assigned to more than one labels -- which is not possible in Weka. \n",
    "\n",
    "The problem of complexity of sklearn is addressed by a different wrapper package called PyCaret that was released in Apr 2020. Everything you did above for sklearn can be accomplished by the following five lines of code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using PyCaret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret import classification\n",
    "numFeat = list(s.columns[5:])\n",
    "base = classification.setup(s, target='Category',ignore_features=['#LMID','Name','Subcategory','Formula'], \n",
    "                            numeric_features=numFeat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_rf = classification.create_model('rf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_rf = classification.tune_model(base_rf, n_iter=5, optimize = 'AUC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification.evaluate_model(tuned_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Look out for large datasets and relevant classification questions where ML can be applied! Let us know too, so that we can use those datasets in future classes!\n",
    "\n",
    "In the next class, we will learn a bit about Deep Learning, and apply PyCaret for solving more machine learning problems. \n",
    "\n",
    "Happy coding!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
